#!/usr/bin/env python

# script to harvest and prep entire dataset, start to finish

import argparse
import glob
import os

from belfastdata.harvest import HarvestRdf, HarvestRelated
from belfastdata.qub import QUB
from belfastdata.clean import SmushGroupSheets, IdentifyGroupSheets
from belfastdata.nx import Rdf2Gexf

# settings

BASE_DIR = os.path.dirname(os.path.abspath(__file__))

harvest_urls = [
    # for now, harvest from test FA site, and only harvest
    # documents with tagged names
    'http://testfindingaids.library.emory.edu/documents/longley744/',
    'http://testfindingaids.library.emory.edu/documents/ormsby805/',
    'http://testfindingaids.library.emory.edu/documents/irishmisc794/',
    'http://testfindingaids.library.emory.edu/documents/carson746/',

    # NOTE: once in production, related collections links should help


    # RDF from TEI group sheets (local dev urls for now)
    'http://localhost:8000/groupsheets/longley1_10244/',
    'http://localhost:8000/groupsheets/longley1_10353/',
    'http://localhost:8000/groupsheets/heaney1_10407/',
    'http://localhost:8000/groupsheets/carson1_1035/',
    'http://localhost:8000/groupsheets/longley1_10202/',
    'http://localhost:8000/groupsheets/heaney1_10415/',
    'http://localhost:8000/groupsheets/heaney1_10365/',
    'http://localhost:8000/groupsheets/heaney1_10163/',
    'http://localhost:8000/groupsheets/heaney1_10199/',
    'http://localhost:8000/groupsheets/heaney1_10236/',
    'http://localhost:8000/groupsheets/heaney1_10269/',
    'http://localhost:8000/groupsheets/longley1_1042/',
    'http://localhost:8000/groupsheets/heaney1_10442/',
    'http://localhost:8000/groupsheets/hobsbaum1_1040/',
    'http://localhost:8000/groupsheets/heaney1_10116/',
    'http://localhost:8000/groupsheets/heaney1_1078/',
    'http://localhost:8000/groupsheets/heaney1_1041/',
    'http://localhost:8000/groupsheets/muldoon2_10121/',
    'http://localhost:8000/groupsheets/muldoon2_1079/',
    'http://localhost:8000/groupsheets/muldoon2_1040/',
    'http://localhost:8000/groupsheets/longley1_10120/',
    'http://localhost:8000/groupsheets/longley1_10158/',
    'http://localhost:8000/groupsheets/longley1_1079/',
    'http://localhost:8000/groupsheets/longley1_10282/',
    'http://localhost:8000/groupsheets/longley1_10316/',
    'http://localhost:8000/groupsheets/simmons1_1035/',
    'http://localhost:8000/groupsheets/simmons1_1069/',
    'http://localhost:8000/groupsheets/hobsbaum1_1047/'

]


# build relative to script or current directory?
output_dir = 'data'

QUB_input = os.path.join(BASE_DIR, '..', 'belfastdata', 'QUB_ms1204.html')

gexf_file = os.path.join(output_dir, 'belfastgroup.gexf')

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Harvest and prep Belfast Group RDF dataset')
    steps = parser.add_argument_group(
        'steps',
        'Data processing steps to run.  Specify one or more to only run those ' +
        'steps.  If not are specified, all steps will be run in order.')
    steps.add_argument('-v', '--harvest', action='store_true',
                       help='Harvest RDFa')
    steps.add_argument('-q', '--queens', action='store_true',
                       help='Convert Queens University Belfast collection to RDF')
    steps.add_argument('-r', '--related', action='store_true',
                       help='Harvest related RDF from VIAF, GeoNames, and DBpedia')
    steps.add_argument('-i', '--infer', action='store_true',
                       help='Identify group sheets and infer connections such as owner')
    steps.add_argument('-s', '--smush', action='store_true',
                       help='Smush groupsheet URIs')
    steps.add_argument('-g', '--gexf', action='store_true',
                       help='Generate GEXF network graph data')
    # TODO: configurable verbosity ?
    # parser.add_argument('-v', '--verbosity', metavar='VERBOSITY', type=int,
    #                     choices=[0, 1, 2], default=1,
    #                     help='Verbosity level; 0=minimal, 1=normal, 2=verbose')

    # TODO: add an option to specify where the output should be generated

    args = parser.parse_args()
    # if specific steps are specified, run only those
    # otherwise, run all steps
    all_steps = not any([args.harvest, args.queens, args.related, args.smush,
                         args.gexf, args.infer])

    if all_steps or args.harvest:
        print '-- Harvesting RDF from EmoryFindingAids related to the Belfast Group'
        HarvestRdf(harvest_urls, output_dir=output_dir,
                   find_related=True, verbosity=0)

    if all_steps or args.queens:
        print '-- Converting Queens University Belfast Group collection description to RDF'
        QUB(QUB_input, output_dir, verbosity=0)

    # files needed for harvest related, smushing, and gexf
    files = glob.glob(os.path.join(output_dir, '*.xml'))
    # NOTE: can't use iglob here if we want to use the same list in multiple steps
    # if we ever get so many files we need that, will have to get the list
    # for each step individually

    if all_steps or args.related:
        print '-- Harvesting related RDF from VIAF, GeoNames, and DBpedia'
        # TODO: add logic so unchanged content isn't re-downloaded
        # (use last-modified http header & file date)
        HarvestRelated(files, output_dir)

    if all_steps or args.infer:
        # smush any groupsheets in the data
        print '-- Inferring: identifying groupsheets'
        IdentifyGroupSheets(files)

    if all_steps or args.smush:
        # smush any groupsheets in the data
        print '-- Smushing groupsheet URIs'
        SmushGroupSheets(files)

    if all_steps or args.gexf:
        # generate gexf
        print '-- Generating network graph and saving as GEXF'
        Rdf2Gexf(files, gexf_file)
